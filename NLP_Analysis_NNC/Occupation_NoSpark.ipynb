{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction:-NLP-Learning-for-Job-Descriptions\" data-toc-modified-id=\"Introduction:-NLP-Learning-for-Job-Descriptions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction: NLP Learning for Job Descriptions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Dataset</a></span></li><li><span><a href=\"#Python-Library\" data-toc-modified-id=\"Python-Library-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Python Library</a></span></li></ul></li><li><span><a href=\"#Data-Set-Loading-and-Cleaning-Up\" data-toc-modified-id=\"Data-Set-Loading-and-Cleaning-Up-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Set Loading and Cleaning Up</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-Job-Description-CSV-Data\" data-toc-modified-id=\"Load-Job-Description-CSV-Data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Load Job Description CSV Data</a></span></li><li><span><a href=\"#Clean-Up\" data-toc-modified-id=\"Clean-Up-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Clean Up</a></span></li></ul></li><li><span><a href=\"#Text-Feature-Engineering\" data-toc-modified-id=\"Text-Feature-Engineering-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Text Feature Engineering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tokenization\" data-toc-modified-id=\"Tokenization-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Tokenization</a></span></li><li><span><a href=\"#Stopword-Removal\" data-toc-modified-id=\"Stopword-Removal-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Stopword Removal</a></span></li><li><span><a href=\"#Lemmatization\" data-toc-modified-id=\"Lemmatization-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Lemmatization</a></span></li><li><span><a href=\"#Word-Embedding-Vectors-with-Gensim\" data-toc-modified-id=\"Word-Embedding-Vectors-with-Gensim-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Word Embedding Vectors with Gensim</a></span><ul class=\"toc-item\"><li><span><a href=\"#Word2Vec-Mean-Vector\" data-toc-modified-id=\"Word2Vec-Mean-Vector-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Word2Vec Mean Vector</a></span></li><li><span><a href=\"#Covert-Vectors-into-Columns\" data-toc-modified-id=\"Covert-Vectors-into-Columns-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Covert Vectors into Columns</a></span></li></ul></li></ul></li><li><span><a href=\"#Data-Preparation-for-Training-and-Testing\" data-toc-modified-id=\"Data-Preparation-for-Training-and-Testing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data Preparation for Training and Testing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standard-Scaling\" data-toc-modified-id=\"Standard-Scaling-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Standard Scaling</a></span></li><li><span><a href=\"#Label-Encoding\" data-toc-modified-id=\"Label-Encoding-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Label Encoding</a></span></li><li><span><a href=\"#Data-Splits-for-Training-and-Testing-Data-Sets\" data-toc-modified-id=\"Data-Splits-for-Training-and-Testing-Data-Sets-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Data Splits for Training and Testing Data Sets</a></span></li></ul></li><li><span><a href=\"#Neural-Network-Classification\" data-toc-modified-id=\"Neural-Network-Classification-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Neural Network Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Neural-Network-Multi-class-Classifier-Training\" data-toc-modified-id=\"Neural-Network-Multi-class-Classifier-Training-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Neural Network Multi-class Classifier Training</a></span></li><li><span><a href=\"#Training-Validation\" data-toc-modified-id=\"Training-Validation-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Training Validation</a></span></li></ul></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Summary</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: NLP Learning for Job Descriptions\n",
    "\n",
    "In this notebook, I will demonstrate how to process unstructure text feature data by using natural language processing (NLP) technologies and train a classification model with those text features. I will use NLTK and gensim word2vec to do text feature engineering and then use multi-class neural network classifier to train a classification model.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset is a historical data of job descriptions stored as \"job_descriptions.csv\" file.\n",
    "\n",
    "## Python Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:37:39.663954Z",
     "start_time": "2020-01-28T20:37:39.137372Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pandas and numpy for converting from Spark dataframe into Pandas dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Make the random numbers predictable\n",
    "np.random.seed(42)\n",
    "import multiprocessing\n",
    "cpu_count = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:37:39.670012Z",
     "start_time": "2020-01-28T20:37:39.666594Z"
    }
   },
   "outputs": [],
   "source": [
    "# Allow multiple output/display from one cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:37:41.149078Z",
     "start_time": "2020-01-28T20:37:39.673520Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "    \n",
    "# Stop Word Removal\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:37:41.237895Z",
     "start_time": "2020-01-28T20:37:41.151949Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:37:41.472607Z",
     "start_time": "2020-01-28T20:37:41.240454Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ivan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set Loading and Cleaning Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Job Description CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:37:42.752519Z",
     "start_time": "2020-01-28T20:37:41.475469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 127893 entries, 0 to 127892\n",
      "Data columns (total 9 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   req_guid                127893 non-null  object \n",
      " 1   original_hcs_code       127893 non-null  object \n",
      " 2   original_hcs_level      125881 non-null  object \n",
      " 3   bill_rate               125881 non-null  float64\n",
      " 4   updated_assigned_hcs    127893 non-null  object \n",
      " 5   updated_assigned_level  127893 non-null  int64  \n",
      " 6   level_indicator         127893 non-null  int64  \n",
      " 7   job_title               127892 non-null  object \n",
      " 8   job_description         127893 non-null  object \n",
      "dtypes: float64(1), int64(2), object(6)\n",
      "memory usage: 8.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./golden_training_hcs_15_18_19.csv', header='infer')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:37:42.761728Z",
     "start_time": "2020-01-28T20:37:42.755305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total amount of training data on job descriptions is:  127893\n"
     ]
    }
   ],
   "source": [
    "print(\"The total amount of training data on job descriptions is: \", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:37:42.891681Z",
     "start_time": "2020-01-28T20:37:42.766375Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove any rows without job_description and this may change the index order\n",
    "df.dropna(inplace=True)\n",
    "# need to reset the index since the index will be used to concat the tables\n",
    "df = df.reset_index(drop=True)\n",
    "# check if index is reset\n",
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:37:43.132341Z",
     "start_time": "2020-01-28T20:37:42.895252Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine simply job description and title so that job title is a part of job description\n",
    "df['job_description'] = df['job_title'] + \" \" + df['job_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:37:43.151044Z",
     "start_time": "2020-01-28T20:37:43.134350Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare final data set for next step of text feature engineering\n",
    "df = df[['req_guid', 'updated_assigned_hcs', 'job_title', 'job_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:37:43.213824Z",
     "start_time": "2020-01-28T20:37:43.153345Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Feature Engineering\n",
    "\n",
    "In this step, raw text data will be transformed into feature vectors. I will implement the following steps in order to obtain relevant features from the dataset.\n",
    "\n",
    "* Tokenizing\n",
    "* Remove stop words\n",
    "* Lemmatization (not stem since stemming can reduce the interpretability) \n",
    "* Word Embeddings as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process by dividing the quantity of text into smaller parts called tokens so that each token can be further treated for machine learning purposes. A token can be a character, a word, a sentence or a paragraph. In this notebook, I only consider words as tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can use NLTK word_tokenize function to process the job description field (by removing punctuations \n",
    "and separating words) like below\n",
    "\n",
    "```python \n",
    "df['job_description'] = df.apply(lambda row: word_tokenize(row.job_description), axis=1) \n",
    "```\n",
    "\n",
    "Or I can just use python string split function to separate text since the job description has been cleaned as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:37:46.644909Z",
     "start_time": "2020-01-28T20:37:43.216265Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the job description and title\n",
    "df['job_description'] = df[\"job_description\"].str.lower()\n",
    "df['job_description'] = df[\"job_description\"].str.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:37:46.673093Z",
     "start_time": "2020-01-28T20:37:46.648188Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal\n",
    "\n",
    "A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a NLP program has been programmed to ignore. In this notebook, I will use NLTK stop words dataset to remove any stop words in job description field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:38:14.807481Z",
     "start_time": "2020-01-28T20:37:46.676015Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get stopwords list from NLTK library\n",
    "stop_words = stopwords.words('english')\n",
    "# Define a function to remove any stop words from input text\n",
    "def removeStopWords(x):\n",
    "        return [w.lower() for w in x if (w not in stop_words) and (w != '') and (w is not None)]\n",
    "# Apply the defined function to remove stop words for job descriptions\n",
    "df['job_description'] = df.apply(lambda row: removeStopWords(row.job_description), axis=1)\n",
    "# Show some results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. For example, in English, the verb 'to walk' may appear as 'walk', 'walked', 'walks', 'walking'. The base form, 'walk', that one might look up in a dictionary, is called the lemma for the word. I will use NLTK lemmatization function to convert words into their lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:39:21.445736Z",
     "start_time": "2020-01-28T20:38:14.809748Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define lemmatization function by using NLTK WordNetLemmatizer function\n",
    "def lemma(x):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w.lower(), pos='v') for w in x if (w != '') and (w is not None)]\n",
    "# Apply the defined function to process job descriptions\n",
    "df['job_description'] = df.apply(lambda row: lemma(row.job_description), axis=1)\n",
    "# Show some results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding Vectors with Gensim\n",
    "\n",
    "A word embedding is a form of representing words and documents using a dense vector representation. The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used. Word embeddings can be trained using the input texts. One can read more about word embeddings [here](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/) and [here](https://jalammar.github.io/illustrated-word2vec/).\n",
    "\n",
    "In this notebook, I am going to use gensim library Word2Vec functionality to generate word embedding vectors so that I can use those vectors later on to train the classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Mean Vector\n",
    "\n",
    "Gensim Word2Vec will generate a vector (dimension of 300 here) for each word after training based on all job descriptions. So, I will define a function to get average (mean) vectors for a job description.\n",
    "\n",
    "In this project, I will use the pre-train word2vec model which pre-trained on 2.5 million job descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:39:25.874297Z",
     "start_time": "2020-01-28T20:39:21.448502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=381161, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model\n",
    "word2vec_path = '../../Word2Vec_Pretrained/nnc_word2vec.bin'\n",
    "model_w2v = Word2Vec.load(word2vec_path)\n",
    "print(model_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:39:25.882339Z",
     "start_time": "2020-01-28T20:39:25.877005Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to get average (mean) vectors for a job description based on trained word2vec model\n",
    "def get_mean_vectors(words):\n",
    "    # remove out of vocabulary words\n",
    "    words = [word for word in words if word in model_w2v.wv.vocab]\n",
    "    if len(words) >= 1:\n",
    "        return np.mean(model_w2v[words], axis=0)\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:40:24.653742Z",
     "start_time": "2020-01-28T20:39:25.884986Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the function to generate 300 dimension vectors for each job description in the data set\n",
    "df['job_description_vectors'] = df.apply(lambda row: get_mean_vectors(row.job_description), axis=1) \n",
    "# Show some sample results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T19:01:12.546860Z",
     "start_time": "2020-01-28T19:01:12.533333Z"
    }
   },
   "source": [
    "The following codes can be used to train a custom word2vec word embedding model when needed.\n",
    "\n",
    "```python\n",
    "# Prepare all the text input for training word2vec model\n",
    "sentences = df['job_description'].tolist()\n",
    "# Define and train a word2vec model. \n",
    "# Here I set vector dimension size to be 300, window (word distanse) to be 5 \n",
    "# and use all available CPUs for parallel processing\n",
    "model_w2v = Word2Vec(sentences, size=300, window=5, min_count=1, workers=cpu_count)\n",
    "# summarize vocabulary\n",
    "# word_vocabulary = list(model_w2v.wv.vocab)\n",
    "# print(word_vocabulary)\n",
    "# save model with binary format\n",
    "model_w2v.save('nnc_word2vec.pkl')\n",
    "# load model when needed so that this word2vec model doesn't need to be re-trained\n",
    "# model_w2v = Word2Vec.load('nnc_word2vec.pkl')\n",
    "print(model_w2v)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, I can use gensim library Doc2Vec to train a Doc2Vec model and then generate vectors as well.  However, I don't choose this method due to 1). relatively too large files that Doc2Vec function will produce (see the screen shot below) and 2). better to be used for documentation simiarlity comparison. Any interested in the discussion on Word2Vec vs. Doc2Vec, it can be refer to this [link](https://datascience.stackexchange.com/questions/20076/word2vec-vs-sentence2vec-vs-doc2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Title 1](./Capture.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T19:03:25.412689Z",
     "start_time": "2020-01-28T19:03:25.409176Z"
    }
   },
   "source": [
    "The training Doc2Vec model and generating vectors from it can be seen in the following codes:\n",
    "```python\n",
    "# Prepare tagged documents for training\n",
    "descriptions = df['job_description'].tolist()\n",
    "tags = df['req_guid'].tolist()\n",
    "docs = []\n",
    "for i in range(len(tags)):\n",
    "    docs.append(doc2vec.TaggedDocument(words=descriptions[i], tags=[\"Train-\" + str(tags[i])]))\n",
    "    \n",
    "docs[:1]\n",
    "\n",
    "# # Setup doc2vec model which is similar to word2vec model setup\n",
    "model_d2v = doc2vec.Doc2Vec(dm=0, vector_size=100, window=5, min_count=1, workers=cpu_count)\n",
    "# # summarize vocabulary\n",
    "model_d2v.build_vocab(docs)\n",
    "# # train model\n",
    "model_d2v.train(docs, total_examples=model_d2v.corpus_count, epochs=model_d2v.epochs)\n",
    "# # save model with binary format\n",
    "model_d2v.save('doc2vec.pkl')\n",
    "\n",
    "# # Load doc2vec model when needed so that the model don't have to be re-trained\n",
    "model_d2v = doc2vec.Doc2Vec.load('doc2vec.pkl')\n",
    "\n",
    "# # Get the job description vectors from the trained doc2vec model by using infer_vector function\n",
    "df['job_description_vectors'] = df.apply(lambda row: model_d2v.infer_vector(row.job_description), axis=1)\n",
    "# # or directly retrieve from tagged documents\n",
    "df['job_description_vectors'] = df.apply(lambda row: model_d2v.docvecs[\"Train-\" + str(row.req_guid)], axis=1) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covert Vectors into Columns\n",
    "\n",
    "Here, I am going to convert job description vectors into columns so that I can easily generate training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:40:25.921046Z",
     "start_time": "2020-01-28T20:40:24.656874Z"
    }
   },
   "outputs": [],
   "source": [
    "# Put job description vectors as columns\n",
    "series = df['job_description_vectors'].apply(lambda x : np.array(x)).values.reshape(-1,1)\n",
    "w2v = np.apply_along_axis(lambda x : x[0], 1, series)\n",
    "w2v_df = pd.DataFrame(w2v)\n",
    "final_df = pd.concat([df, w2v_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:40:25.960236Z",
     "start_time": "2020-01-28T20:40:25.923260Z"
    }
   },
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:40:26.322495Z",
     "start_time": "2020-01-28T20:40:25.962847Z"
    }
   },
   "outputs": [],
   "source": [
    "# Name label field so that the classification model knows which field is the target (label) one\n",
    "final_df.rename(columns={'updated_assigned_hcs':'label'}, inplace=True)\n",
    "# Prepare final data table by removing unuseful columns\n",
    "final_df.drop(columns=['req_guid','job_title', 'job_description', 'job_description_vectors'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:40:26.769854Z",
     "start_time": "2020-01-28T20:40:26.324803Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove any NULL or blank rows\n",
    "final_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:40:26.802410Z",
     "start_time": "2020-01-28T20:40:26.776797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00-000</td>\n",
       "      <td>-0.086361</td>\n",
       "      <td>-0.294860</td>\n",
       "      <td>-0.699821</td>\n",
       "      <td>0.102510</td>\n",
       "      <td>-0.081705</td>\n",
       "      <td>0.608750</td>\n",
       "      <td>-0.724328</td>\n",
       "      <td>-0.128486</td>\n",
       "      <td>0.835863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342909</td>\n",
       "      <td>-0.746965</td>\n",
       "      <td>0.324627</td>\n",
       "      <td>-0.347712</td>\n",
       "      <td>0.166916</td>\n",
       "      <td>0.406738</td>\n",
       "      <td>-0.799520</td>\n",
       "      <td>0.077131</td>\n",
       "      <td>0.359275</td>\n",
       "      <td>0.287279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00-000</td>\n",
       "      <td>-0.173583</td>\n",
       "      <td>1.278914</td>\n",
       "      <td>-1.200068</td>\n",
       "      <td>0.564437</td>\n",
       "      <td>0.263772</td>\n",
       "      <td>-0.438466</td>\n",
       "      <td>0.271884</td>\n",
       "      <td>-0.734823</td>\n",
       "      <td>-0.538172</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.337122</td>\n",
       "      <td>0.506212</td>\n",
       "      <td>-0.369966</td>\n",
       "      <td>-0.994712</td>\n",
       "      <td>0.451871</td>\n",
       "      <td>0.017072</td>\n",
       "      <td>-0.211761</td>\n",
       "      <td>-0.362503</td>\n",
       "      <td>0.997578</td>\n",
       "      <td>0.576597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00-000</td>\n",
       "      <td>0.673318</td>\n",
       "      <td>-0.032487</td>\n",
       "      <td>0.432261</td>\n",
       "      <td>-0.508771</td>\n",
       "      <td>0.476943</td>\n",
       "      <td>0.291932</td>\n",
       "      <td>-0.509735</td>\n",
       "      <td>0.862125</td>\n",
       "      <td>-0.903905</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.135338</td>\n",
       "      <td>1.484898</td>\n",
       "      <td>-0.376700</td>\n",
       "      <td>-0.514329</td>\n",
       "      <td>0.851197</td>\n",
       "      <td>-0.124478</td>\n",
       "      <td>0.088335</td>\n",
       "      <td>-0.612183</td>\n",
       "      <td>-0.592375</td>\n",
       "      <td>0.263966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00-000</td>\n",
       "      <td>0.120055</td>\n",
       "      <td>0.150500</td>\n",
       "      <td>0.023888</td>\n",
       "      <td>0.623277</td>\n",
       "      <td>-0.154826</td>\n",
       "      <td>-0.322433</td>\n",
       "      <td>-1.134776</td>\n",
       "      <td>0.638786</td>\n",
       "      <td>-0.339165</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049732</td>\n",
       "      <td>0.486116</td>\n",
       "      <td>0.528126</td>\n",
       "      <td>0.541076</td>\n",
       "      <td>0.179810</td>\n",
       "      <td>-0.529596</td>\n",
       "      <td>0.055889</td>\n",
       "      <td>-0.019389</td>\n",
       "      <td>-0.363229</td>\n",
       "      <td>0.296873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00-000</td>\n",
       "      <td>0.714245</td>\n",
       "      <td>0.083796</td>\n",
       "      <td>-0.238266</td>\n",
       "      <td>-0.223597</td>\n",
       "      <td>0.355629</td>\n",
       "      <td>0.173768</td>\n",
       "      <td>-0.725232</td>\n",
       "      <td>0.296629</td>\n",
       "      <td>-0.313183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235431</td>\n",
       "      <td>0.755302</td>\n",
       "      <td>0.291932</td>\n",
       "      <td>0.643426</td>\n",
       "      <td>-0.024221</td>\n",
       "      <td>0.418336</td>\n",
       "      <td>1.028122</td>\n",
       "      <td>-1.041257</td>\n",
       "      <td>0.196144</td>\n",
       "      <td>-1.213655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label         0         1         2         3         4         5  \\\n",
       "0  00-000 -0.086361 -0.294860 -0.699821  0.102510 -0.081705  0.608750   \n",
       "1  00-000 -0.173583  1.278914 -1.200068  0.564437  0.263772 -0.438466   \n",
       "2  00-000  0.673318 -0.032487  0.432261 -0.508771  0.476943  0.291932   \n",
       "3  00-000  0.120055  0.150500  0.023888  0.623277 -0.154826 -0.322433   \n",
       "4  00-000  0.714245  0.083796 -0.238266 -0.223597  0.355629  0.173768   \n",
       "\n",
       "          6         7         8  ...       290       291       292       293  \\\n",
       "0 -0.724328 -0.128486  0.835863  ...  0.342909 -0.746965  0.324627 -0.347712   \n",
       "1  0.271884 -0.734823 -0.538172  ... -0.337122  0.506212 -0.369966 -0.994712   \n",
       "2 -0.509735  0.862125 -0.903905  ... -0.135338  1.484898 -0.376700 -0.514329   \n",
       "3 -1.134776  0.638786 -0.339165  ... -0.049732  0.486116  0.528126  0.541076   \n",
       "4 -0.725232  0.296629 -0.313183  ...  0.235431  0.755302  0.291932  0.643426   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0  0.166916  0.406738 -0.799520  0.077131  0.359275  0.287279  \n",
       "1  0.451871  0.017072 -0.211761 -0.362503  0.997578  0.576597  \n",
       "2  0.851197 -0.124478  0.088335 -0.612183 -0.592375  0.263966  \n",
       "3  0.179810 -0.529596  0.055889 -0.019389 -0.363229  0.296873  \n",
       "4 -0.024221  0.418336  1.028122 -1.041257  0.196144 -1.213655  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show some sample results\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:40:27.150690Z",
     "start_time": "2020-01-28T20:40:26.805248Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get numpy array on feature set X and target set y\n",
    "X = np.array(final_df.drop(columns=['label']))\n",
    "\n",
    "y = np.array(final_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:40:27.160745Z",
     "start_time": "2020-01-28T20:40:27.153473Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125880, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(125880,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double check their shape\n",
    "X.shape\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaling\n",
    "\n",
    "Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. neural network classifier). People usually can standardize features by removing the mean and scaling to unit variance. In this notebook, I will use StandardScaler function to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:40:28.296443Z",
     "start_time": "2020-01-28T20:40:27.163474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15307766, -1.0897384 , -1.6942576 , ...,  0.3473458 ,\n",
       "         0.91302884,  0.58050215],\n",
       "       [-0.03016952,  2.9683032 , -2.804353  , ..., -0.5940591 ,\n",
       "         2.404998  ,  1.1093311 ],\n",
       "       [ 1.7491156 , -0.41319907,  0.81794024, ..., -1.128707  ,\n",
       "        -1.3113598 ,  0.5378902 ],\n",
       "       ...,\n",
       "       [-0.14342314, -0.18321522, -1.2022563 , ..., -0.05808655,\n",
       "         3.3263485 , -0.6350132 ],\n",
       "       [ 0.15869135,  0.35835943, -0.33742136, ...,  0.84332585,\n",
       "        -1.1395992 ,  1.2240493 ],\n",
       "       [ 0.39118102, -0.6659653 , -1.193912  , ...,  0.45354998,\n",
       "        -0.08339486,  1.8141719 ]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standardize the vectors\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:40:28.303466Z",
     "start_time": "2020-01-28T20:40:28.298846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00-000', '00-000', '00-000', ..., '00-000', '00-000', '00-000'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding\n",
    "\n",
    "Holds the label for each class. Encode categorical features using a one-hot or ordinal encoding scheme. It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels. In this notebook, I am going to use LabelEncoder function to generate indexes for the target label field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:40:28.337614Z",
     "start_time": "2020-01-28T20:40:28.306055Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding the lebels\n",
    "tmp = y\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:40:28.398746Z",
     "start_time": "2020-01-28T20:40:28.340877Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the mapping between index and labels so that I can refer back to labels when get predicted results.\n",
    "df_label = pd.DataFrame({'label':tmp, 'label_index':y})\n",
    "df_label.drop_duplicates().to_csv('./hcs_label_index.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splits for Training and Testing Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:40:28.856550Z",
     "start_time": "2020-01-28T20:40:28.400641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100704\n",
      "25176\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Multi-class Classifier Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:40:28.864603Z",
     "start_time": "2020-01-28T20:40:28.858774Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a neural netowork multi-class classifier model\n",
    "# Two hidden layers are set to 300 and 200\n",
    "# Activation function is set to relu\n",
    "# Solver (optimizer) function is set to adam for faster converge and better performance (lbfgs solver/optimizer causes overfitting)\n",
    "# Early_stopping is set to true to prevent overfitting\n",
    "# Learning rate is set to adaptive, which means it will automatically adjust from initial rate during each iteration\n",
    "# Alpha is set to 0.01 so that some degree of overfitting can be prevented\n",
    "# Batch_size is set to 128 so that 128 training data instances are used for calculation gradient descent\n",
    "# Random_state is set to 42, consistant with initial setup\n",
    "# Verbose is set to true to display information for all iterations\n",
    "est = MLPClassifier(activation='relu', \\\n",
    "                    hidden_layer_sizes=(300, 200),\\\n",
    "                    solver='adam',\\\n",
    "                    learning_rate='adaptive',\\\n",
    "                    max_iter=500,\\\n",
    "                    learning_rate_init=0.001,\\\n",
    "                    batch_size=128,\\\n",
    "                    tol=0.000001,\\\n",
    "                    random_state=42,\\\n",
    "                    early_stopping=True,\\\n",
    "                    verbose=True,\\\n",
    "                    alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:44:22.408982Z",
     "start_time": "2020-01-28T20:40:28.867177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Neural Network MLP-Classifier...\n",
      "Iteration 1, loss = 0.23895872\n",
      "Validation score: 0.965247\n",
      "Iteration 2, loss = 0.13646807\n",
      "Validation score: 0.965942\n",
      "Iteration 3, loss = 0.11615888\n",
      "Validation score: 0.969814\n",
      "Iteration 4, loss = 0.10408511\n",
      "Validation score: 0.972098\n",
      "Iteration 5, loss = 0.09497245\n",
      "Validation score: 0.972098\n",
      "Iteration 6, loss = 0.08947659\n",
      "Validation score: 0.971204\n",
      "Iteration 7, loss = 0.08344035\n",
      "Validation score: 0.974084\n",
      "Iteration 8, loss = 0.08043861\n",
      "Validation score: 0.972694\n",
      "Iteration 9, loss = 0.07744376\n",
      "Validation score: 0.974680\n",
      "Iteration 10, loss = 0.07567926\n",
      "Validation score: 0.975573\n",
      "Iteration 11, loss = 0.07334694\n",
      "Validation score: 0.973985\n",
      "Iteration 12, loss = 0.07051221\n",
      "Validation score: 0.975871\n",
      "Iteration 13, loss = 0.06924813\n",
      "Validation score: 0.974978\n",
      "Iteration 14, loss = 0.06974275\n",
      "Validation score: 0.973091\n",
      "Iteration 15, loss = 0.06719382\n",
      "Validation score: 0.975375\n",
      "Iteration 16, loss = 0.06495366\n",
      "Validation score: 0.975871\n",
      "Iteration 17, loss = 0.06813549\n",
      "Validation score: 0.976169\n",
      "Iteration 18, loss = 0.06454030\n",
      "Validation score: 0.972495\n",
      "Iteration 19, loss = 0.06187835\n",
      "Validation score: 0.975971\n",
      "Iteration 20, loss = 0.06313603\n",
      "Validation score: 0.976666\n",
      "Iteration 21, loss = 0.06163290\n",
      "Validation score: 0.971304\n",
      "Iteration 22, loss = 0.06099257\n",
      "Validation score: 0.976864\n",
      "Iteration 23, loss = 0.06210684\n",
      "Validation score: 0.971204\n",
      "Iteration 24, loss = 0.05983280\n",
      "Validation score: 0.976666\n",
      "Iteration 25, loss = 0.05931625\n",
      "Validation score: 0.974183\n",
      "Iteration 26, loss = 0.05964439\n",
      "Validation score: 0.974878\n",
      "Iteration 27, loss = 0.05791724\n",
      "Validation score: 0.973985\n",
      "Iteration 28, loss = 0.05635361\n",
      "Validation score: 0.977758\n",
      "Iteration 29, loss = 0.06157399\n",
      "Validation score: 0.974878\n",
      "Iteration 30, loss = 0.05571369\n",
      "Validation score: 0.975474\n",
      "Iteration 31, loss = 0.05709855\n",
      "Validation score: 0.975573\n",
      "Iteration 32, loss = 0.05575122\n",
      "Validation score: 0.975673\n",
      "Iteration 33, loss = 0.05663162\n",
      "Validation score: 0.976070\n",
      "Iteration 34, loss = 0.05603433\n",
      "Validation score: 0.976964\n",
      "Iteration 35, loss = 0.05541284\n",
      "Validation score: 0.975871\n",
      "Iteration 36, loss = 0.05271885\n",
      "Validation score: 0.977361\n",
      "Iteration 37, loss = 0.05611389\n",
      "Validation score: 0.974878\n",
      "Iteration 38, loss = 0.05338500\n",
      "Validation score: 0.976964\n",
      "Iteration 39, loss = 0.05216936\n",
      "Validation score: 0.972595\n",
      "Validation score did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "MLPClassifier(activation='relu', alpha=0.01, batch_size=128, beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(300, 200), learning_rate='adaptive',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=42, shuffle=True, solver='adam',\n",
      "              tol=1e-06, validation_fraction=0.1, verbose=True,\n",
      "              warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Neural Network MLP-Classifier...\")\n",
    "nnc = est.fit(X_train, y_train)\n",
    "print(nnc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:44:24.989308Z",
     "start_time": "2020-01-28T20:44:22.411732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training Data are:  0.9932475373371464\n"
     ]
    }
   ],
   "source": [
    "# Check classification accuracy on training data\n",
    "print(\"Accuracy on Training Data are: \", accuracy_score(nnc.predict(X_train), y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:44:25.651611Z",
     "start_time": "2020-01-28T20:44:24.992213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Testing Data are:  0.9766047028916428\n"
     ]
    }
   ],
   "source": [
    "# Check classification accuracy on testing data \n",
    "print(\"Accuracy on Testing Data are: \", accuracy_score(nnc.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:44:26.350634Z",
     "start_time": "2020-01-28T20:44:25.653897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     21509\n",
      "           1       0.92      0.94      0.93       605\n",
      "           2       1.00      0.40      0.57        25\n",
      "           3       0.80      0.88      0.84       152\n",
      "           4       0.86      0.63      0.73       125\n",
      "           5       0.47      0.62      0.53        34\n",
      "           6       0.89      0.85      0.87        20\n",
      "           7       0.92      0.91      0.91        96\n",
      "           8       0.94      0.89      0.92        74\n",
      "           9       0.78      0.95      0.86        88\n",
      "          10       0.95      0.95      0.95       286\n",
      "          11       0.92      0.89      0.91        82\n",
      "          12       0.33      0.50      0.40         2\n",
      "          13       0.50      0.50      0.50         6\n",
      "          14       0.79      0.67      0.73        55\n",
      "          15       0.90      0.80      0.85       111\n",
      "          16       0.94      0.71      0.81        42\n",
      "          17       1.00      0.50      0.67         8\n",
      "          18       0.73      0.88      0.80       112\n",
      "          19       0.93      0.86      0.90        96\n",
      "          20       1.00      0.75      0.86         4\n",
      "          21       0.00      0.00      0.00         4\n",
      "          22       1.00      0.89      0.94         9\n",
      "          23       0.83      0.62      0.71        84\n",
      "          24       0.86      0.91      0.88       204\n",
      "          25       1.00      0.56      0.72        16\n",
      "          26       0.92      0.93      0.92       619\n",
      "          27       1.00      1.00      1.00         7\n",
      "          28       0.89      0.65      0.75        37\n",
      "          29       0.85      0.76      0.80       103\n",
      "          30       1.00      0.86      0.92         7\n",
      "          31       0.99      0.97      0.98       230\n",
      "          32       0.82      0.82      0.82        17\n",
      "          33       0.93      0.95      0.94        44\n",
      "          34       1.00      0.67      0.80         3\n",
      "          35       0.97      0.83      0.89       214\n",
      "          36       1.00      1.00      1.00         3\n",
      "          37       1.00      1.00      1.00         3\n",
      "          38       0.90      1.00      0.95         9\n",
      "          39       0.85      0.94      0.89        31\n",
      "\n",
      "    accuracy                           0.98     25176\n",
      "   macro avg       0.86      0.79      0.81     25176\n",
      "weighted avg       0.98      0.98      0.98     25176\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Check classification (confusion matrix) report on testing data set\n",
    "print(classification_report(y_test, nnc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:48:47.666550Z",
     "start_time": "2020-01-28T20:44:26.353008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Neural Network MLP-Classifier on whole data set...\n",
      "Iteration 1, loss = 0.22232934\n",
      "Validation score: 0.963854\n",
      "Iteration 2, loss = 0.13176461\n",
      "Validation score: 0.968462\n",
      "Iteration 3, loss = 0.11196658\n",
      "Validation score: 0.963298\n",
      "Iteration 4, loss = 0.10196866\n",
      "Validation score: 0.971163\n",
      "Iteration 5, loss = 0.09240039\n",
      "Validation score: 0.972196\n",
      "Iteration 6, loss = 0.08956471\n",
      "Validation score: 0.970448\n",
      "Iteration 7, loss = 0.08350056\n",
      "Validation score: 0.972593\n",
      "Iteration 8, loss = 0.07999116\n",
      "Validation score: 0.974182\n",
      "Iteration 9, loss = 0.07834055\n",
      "Validation score: 0.971322\n",
      "Iteration 10, loss = 0.07666497\n",
      "Validation score: 0.975056\n",
      "Iteration 11, loss = 0.07272874\n",
      "Validation score: 0.974420\n",
      "Iteration 12, loss = 0.07298363\n",
      "Validation score: 0.974738\n",
      "Iteration 13, loss = 0.07129260\n",
      "Validation score: 0.975612\n",
      "Iteration 14, loss = 0.06935033\n",
      "Validation score: 0.975691\n",
      "Iteration 15, loss = 0.06793387\n",
      "Validation score: 0.973308\n",
      "Iteration 16, loss = 0.06860305\n",
      "Validation score: 0.976565\n",
      "Iteration 17, loss = 0.06635083\n",
      "Validation score: 0.975294\n",
      "Iteration 18, loss = 0.06570139\n",
      "Validation score: 0.972672\n",
      "Iteration 19, loss = 0.06445148\n",
      "Validation score: 0.975056\n",
      "Iteration 20, loss = 0.06382018\n",
      "Validation score: 0.975691\n",
      "Iteration 21, loss = 0.06357845\n",
      "Validation score: 0.976247\n",
      "Iteration 22, loss = 0.06429997\n",
      "Validation score: 0.977359\n",
      "Iteration 23, loss = 0.06179688\n",
      "Validation score: 0.976883\n",
      "Iteration 24, loss = 0.06104667\n",
      "Validation score: 0.975929\n",
      "Iteration 25, loss = 0.06351075\n",
      "Validation score: 0.975929\n",
      "Iteration 26, loss = 0.05907979\n",
      "Validation score: 0.977359\n",
      "Iteration 27, loss = 0.05992650\n",
      "Validation score: 0.976486\n",
      "Iteration 28, loss = 0.05958873\n",
      "Validation score: 0.974579\n",
      "Iteration 29, loss = 0.05820538\n",
      "Validation score: 0.975850\n",
      "Iteration 30, loss = 0.05658194\n",
      "Validation score: 0.976962\n",
      "Iteration 31, loss = 0.05948166\n",
      "Validation score: 0.974658\n",
      "Iteration 32, loss = 0.05757934\n",
      "Validation score: 0.976406\n",
      "Iteration 33, loss = 0.05576104\n",
      "Validation score: 0.976883\n",
      "Validation score did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "MLPClassifier(activation='relu', alpha=0.01, batch_size=128, beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(300, 200), learning_rate='adaptive',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=42, shuffle=True, solver='adam',\n",
      "              tol=1e-06, validation_fraction=0.1, verbose=True,\n",
      "              warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Neural Network MLP-Classifier on whole data set...\")\n",
    "nnc = est.fit(X, y)\n",
    "print(nnc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:48:47.688632Z",
     "start_time": "2020-01-28T20:48:47.669692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./nnc.pkl']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model into a file so that it can be used later on without re-training the model\n",
    "joblib.dump(nnc, './nnc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T20:48:48.379181Z",
     "start_time": "2020-01-28T20:48:47.690695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Testing Data are:  0.992333968859231\n"
     ]
    }
   ],
   "source": [
    "# Load the classification model\n",
    "loaded_model = joblib.load('./nnc.pkl')\n",
    "# Predict and score the data set with the loaded model\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "print(\"Accuracy on Testing Data are: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook I went through major steps to demonstrate how a NLP data science project can be implemented. \n",
    "\n",
    "During this project implementation, I used pandas, gensim and scikit-learn libraries running on a local machine. That is, the running time can be an exponential growth on the size of training data set. This issue can be addressed by using multiprocessing or dask libraries to enhance parallel processing.\n",
    "\n",
    "Another scalable solution for this problem is to use Spark (with pyspark library) on a Hadoop cluster (a group of multiple servers). However, it also means the NLP models will be depended on the spark distributed environment. So, this approach is probably only applicable or feasible when a big data volume of training data set needs to be considered."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "371.141px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
